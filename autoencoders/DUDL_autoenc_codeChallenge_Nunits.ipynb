{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 19/November/2022 - Saturday\n",
    "# Author: Virgilio Murillo Ochoa\n",
    "# personal github: Virgilio-AI\n",
    "# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203\n",
    "# contact: virgiliomurilloochoa1@gmail.com\n",
    "# web: virgiliomurillo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# NEW!\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80ebd4",
   "metadata": {},
   "source": [
    "# Import and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset (comes with colab!)\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# don't need labels!\n",
    "data = data[:,1:]\n",
    "\n",
    "# normalize the data to a range of [0 1]\n",
    "dataNorm = data / np.max(data)\n",
    "\n",
    "# convert to tensor\n",
    "dataT = torch.tensor( dataNorm ).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b106bef5",
   "metadata": {},
   "source": [
    "# Create the DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "def createTheMNISTAE(n_enc,n_bottle):\n",
    "\n",
    "  class aenet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      ### input layer\n",
    "      self.input = nn.Linear(784,n_enc)\n",
    "      \n",
    "      ### encoder layer\n",
    "      self.encoding = nn.Linear(n_enc,n_bottle)\n",
    "\n",
    "      ### bottleneck layer\n",
    "      self.bottleneck = nn.Linear(n_bottle,n_enc)\n",
    "\n",
    "      ### decoder layer\n",
    "      self.decoding = nn.Linear(n_enc,784)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self,x):\n",
    "      x = F.relu( self.input(x) )\n",
    "      x = F.relu( self.encoding(x) )\n",
    "      x = F.relu( self.bottleneck(x) )\n",
    "      y = torch.sigmoid( self.decoding(x) )\n",
    "      return y\n",
    "  \n",
    "  # create the model instance\n",
    "  net = aenet()\n",
    "  \n",
    "  # loss function\n",
    "  lossfun = nn.MSELoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.Adam(net.parameters(),lr=.001)\n",
    "\n",
    "  return net,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fca6d5",
   "metadata": {},
   "source": [
    "# Create a function that trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d5e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the difference in selecting samples compared to DUDL_autoenc_denoisingMNIST\n",
    "\n",
    "def function2trainTheModel(n_enc,n_bottle):\n",
    "\n",
    "  # number of epochs\n",
    "  numepochs = 3\n",
    "  \n",
    "  # create a new model\n",
    "  net,lossfun,optimizer = createTheMNISTAE(n_enc,n_bottle)\n",
    "\n",
    "  # initialize losses\n",
    "  losses = []\n",
    "\n",
    "\n",
    "  # batch size and number of batches\n",
    "  batchsize  = 32\n",
    "  numBatches = int(dataT.shape[0]/batchsize)\n",
    "\n",
    "\n",
    "  # loop over epochs (now each epoch goes through all samples)\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    \n",
    "    # get a permuted index vector\n",
    "    randidx = np.random.permutation(dataT.shape[0]).astype(int)\n",
    "\n",
    "    # losses during the batches\n",
    "    batchlosses = []\n",
    "\n",
    "    for batchi in range(numBatches):\n",
    "      \n",
    "      # samples to use in this batch\n",
    "      samps2use = range((batchi-1)*batchsize,batchi*batchsize)\n",
    "      \n",
    "\n",
    "      # select those images\n",
    "      X = dataT[randidx[samps2use],:]\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = net(X)\n",
    "      loss = lossfun(yHat,X)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    \n",
    "      # losses in this batch\n",
    "      batchlosses.append( loss.item() )\n",
    "    # end minibatch loop\n",
    "  \n",
    "    losses.append( np.mean(batchlosses[-3:]) )\n",
    "    # Note about the above line: This was slightly incorrect in the recording. I said\n",
    "    # to average the last 3 training losses, but the code in the video averaged all\n",
    "    # the batch losses. However, that doesn't change the outcome or the conclusions.\n",
    "  # end epoch loop\n",
    "  \n",
    "  # function output\n",
    "  return losses,net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d468c653",
   "metadata": {},
   "source": [
    "# Run the experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note! this cell takes ~25 minutes with initial N settings\n",
    "\n",
    "# specific the number of units\n",
    "N_encdec_units = np.linspace(10,500,12).astype(int)\n",
    "N_bottle_units = np.linspace(5,100,8).astype(int)\n",
    "\n",
    "\n",
    "# initialize results matrix\n",
    "exp_results = np.zeros((len(N_encdec_units),len(N_bottle_units)))\n",
    "\n",
    "\n",
    "# start the experiment!\n",
    "for ei,nenc in enumerate(N_encdec_units):\n",
    "  for bi,nbot in enumerate(N_bottle_units):\n",
    "\n",
    "    # build/train a model\n",
    "    losses = function2trainTheModel(nenc,nbot)[0] # only need the first output\n",
    "    exp_results[ei,bi] = np.mean(losses[-1])\n",
    "\n",
    "    # send update message\n",
    "    currentIter = ei*len(N_bottle_units)+bi+1\n",
    "    totalIters  = len(N_bottle_units)*len(N_encdec_units)\n",
    "    msg = 'Finished experiment {}/{}'.format(currentIter,totalIters)\n",
    "    sys.stdout.write('\\r' + msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea32f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the results matrix\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.imshow(exp_results,aspect='auto',         # data and aspect ratio\n",
    "           vmin=.01,vmax=.04, cmap='Purples', # color range and palette\n",
    "           extent=[ N_bottle_units[0],N_bottle_units[-1],N_encdec_units[-1],N_encdec_units[0], ]) # xy axis ticks\n",
    "           \n",
    "\n",
    "plt.xlabel('Number of bottleneck units')\n",
    "plt.ylabel('Number of encoder/decoder units')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c1c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perhaps a line plot will better reveal the sudden transition\n",
    "\n",
    "plt.plot(N_encdec_units,exp_results)\n",
    "plt.legend(N_bottle_units,loc=(1.01,0))\n",
    "plt.xlabel('Number of enc/dec units')\n",
    "plt.title('Loss by bottleneck units')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34463135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7c35f97",
   "metadata": {},
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4f009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Because the full experiment takes a long time, it's not pratical to add another factor. Fix the number of encoder\n",
    "#    units to 100 and instead parametrically explore the learning rate. You don't need so many learning rates, just use\n",
    "#    [.0001, .001, .01]. The results can be shown in a line plot, with one line per lr and bottleneck units on the x-axis.\n",
    "# \n",
    "# 2) Smooth transitions across parameters are easy to interpret. But the image plot shows a sharp transition for small\n",
    "#    numbers of bottleneck units. This rings alarm bells for any experimental scientist! It means that something is \n",
    "#    happening at that region of parameter space and you should investigate. Thus, re-run the experiment but change the\n",
    "#    parameters to focus specifically on the region of the parameter space where there are large changes in the results.\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
