{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a73207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 19/November/2022 - Saturday\n",
    "# Author: Virgilio Murillo Ochoa\n",
    "# personal github: Virgilio-AI\n",
    "# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203\n",
    "# contact: virgiliomurilloochoa1@gmail.com\n",
    "# web: virgiliomurillo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable to evaluate over\n",
    "x = torch.linspace(-3,3,101)\n",
    "\n",
    "# create a function that returns the activated output\n",
    "def NNoutputx(actfun):\n",
    "  # get activation function type\n",
    "  # this code replaces torch.relu with torch.<actfun>\n",
    "  actfun = getattr(torch,actfun)\n",
    "  return actfun( x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the activation functions\n",
    "activation_funs = [ 'relu', 'sigmoid', 'tanh' ]\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "for actfun in activation_funs:\n",
    "  plt.plot(x,NNoutputx(actfun),label=actfun,linewidth=3)\n",
    "\n",
    "# add reference lines\n",
    "dashlinecol = [.7,.7,.7]\n",
    "plt.plot(x[[0,-1]],[0,0],'--',color=dashlinecol)\n",
    "plt.plot(x[[0,-1]],[1,1],'--',color=dashlinecol)\n",
    "plt.plot([0,0],[-1,3],'--',color=dashlinecol)\n",
    "\n",
    "# make the plot look nicer\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\sigma(x)$')\n",
    "plt.title('Various activation functions')\n",
    "plt.xlim(x[[0,-1]])\n",
    "plt.ylim([-1,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81541450",
   "metadata": {},
   "source": [
    "# More activation functions in torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836384b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that returns the activated output FUNCTION\n",
    "# this is different from the previous function\n",
    "def NNoutput(actfun):\n",
    "  # get activation function type\n",
    "  # this code replaces torch.nn.relu with torch.nn.<actfun>\n",
    "  actfun = getattr(torch.nn,actfun)\n",
    "  return actfun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55601d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the activation functions\n",
    "activation_funs = [ 'ReLU6', 'Hardshrink', 'LeakyReLU' ]\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "for actfun in activation_funs:\n",
    "  plt.plot(x,NNoutput(actfun)(x),label=actfun,linewidth=3)\n",
    "\n",
    "# add reference lines\n",
    "dashlinecol = [.7,.7,.7]\n",
    "plt.plot(x[[0,-1]],[0,0],'--',color=dashlinecol)\n",
    "plt.plot(x[[0,-1]],[1,1],'--',color=dashlinecol)\n",
    "plt.plot([0,0],[-1,3],'--',color=dashlinecol)\n",
    "\n",
    "# make the plot look nicer\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\sigma(x)$')\n",
    "plt.title('Various activation functions')\n",
    "plt.xlim(x[[0,-1]])\n",
    "plt.ylim([-1,3])\n",
    "# plt.ylim([-.1,.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a263f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu6 in more detail\n",
    "x = torch.linspace(-3,9,101)\n",
    "relu6 = torch.nn.ReLU6()\n",
    "\n",
    "plt.plot(x,relu6(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aec93a",
   "metadata": {},
   "source": [
    "# Differences between torch and torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107789b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine x (fewer points to facilitate visualization)\n",
    "x = torch.linspace(-3,3,21)\n",
    "\n",
    "# in torch\n",
    "y1 = torch.relu(x)\n",
    "\n",
    "# in torch.nn\n",
    "f = torch.nn.ReLU()\n",
    "y2 = f(x)\n",
    "\n",
    "\n",
    "# the results are the same\n",
    "plt.plot(x,y1,'ro',label='torch.relu')\n",
    "plt.plot(x,y2,'bx',label='torch.nn.ReLU')\n",
    "plt.legend()\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b4837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of activation functions in PyTorch:\n",
    "#  https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36556420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d512a98",
   "metadata": {},
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f660a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal of these explorations is to help you appreciate the remarkably diverse nonlinear shapes that a node can produce.\n",
    "# All explorations use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fdd67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input vectors\n",
    "x1 = torch.linspace(-1,1,20)\n",
    "x2 = 2*x1\n",
    "\n",
    "# and corresponding weights\n",
    "w1 = -.3\n",
    "w2 = .5\n",
    "\n",
    "# their linear combination\n",
    "linpart = x1*w1 + x2*w2\n",
    "\n",
    "# and the nonlinear output\n",
    "y = torch.relu(linpart)\n",
    "\n",
    "# and plot!\n",
    "plt.plot(x1,linpart,'bo-',label='Linear input')\n",
    "plt.plot(x1,y,'rs',label='Nonlinear output')\n",
    "plt.ylabel('$\\\\hat{y}$ (output of activation function)')\n",
    "plt.xlabel('x1 variable')\n",
    "# plt.ylim([-.1,.1]) # optional -- uncomment and modify to zoom in\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Look through the code to make sure you understand what it does (linear weighted combination -> nonlinear function).\n",
    "# \n",
    "# 2) Set x2=x1**2 and run the code. Then set one of the weights to be negative. Then set the negative weight to be close\n",
    "#    to zero (e.g., -.01) with the positive weight relatively large (e.g., .8). Then swap the signs.\n",
    "# \n",
    "# 3) Set x2=x1**2, and set the weights to be .4 and .6. Now set w2=.6 (you might want to zoom in on the y-axis).\n",
    "# \n",
    "# 4) Set x2 to be the absolute value of x1 and both weights positive. Then set w2=-.6. Why does w2<0 have such a big impact?\n",
    "#    More generally, under what conditions are the input and output identical? \n",
    "# \n",
    "# 5) Have fun! Spend a few minutes playing around with the code. Also try changing the activation function to tanh or \n",
    "#    anything else. The goal is to see that really simple input functions with really simple weights can produce really\n",
    "#    complicated-looking nonlinear outputs.\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
