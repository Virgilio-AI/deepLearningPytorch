{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7798ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 19/November/2022 - Saturday\n",
    "# Author: Virgilio Murillo Ochoa\n",
    "# personal github: Virgilio-AI\n",
    "# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203\n",
    "# contact: virgiliomurilloochoa1@gmail.com\n",
    "# web: virgiliomurillo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be14c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8293b7a9",
   "metadata": {},
   "source": [
    "# Create the DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7899d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "class thenet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    ### input layer\n",
    "    self.input = nn.Linear(100,100)\n",
    "    \n",
    "    ### hidden layer\n",
    "    self.fc1 = nn.Linear(100,100)\n",
    "    self.fc2 = nn.Linear(100,100)\n",
    "    self.fc3 = nn.Linear(100,100)\n",
    "\n",
    "    ### output layer\n",
    "    self.output = nn.Linear(100,2)\n",
    "\n",
    "  # forward pass\n",
    "  def forward(self,x):\n",
    "    x = F.relu( self.input(x) )\n",
    "    x = F.relu( self.fc1(x) )\n",
    "    x = F.relu( self.fc2(x) )\n",
    "    x = F.relu( self.fc3(x) )\n",
    "    return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed1810",
   "metadata": {},
   "source": [
    "# Explore the initialized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22d2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the model\n",
    "net = thenet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43180b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all weights and biases\n",
    "allweight = np.array([])\n",
    "allbiases = np.array([])\n",
    "\n",
    "for p in net.named_parameters():\n",
    "  if 'bias' in p[0]:\n",
    "    allbiases = np.concatenate( (allbiases,p[1].data.numpy().flatten()),axis=0 )\n",
    "  elif 'weight' in p[0]:\n",
    "    allweight = np.concatenate( (allweight,p[1].data.numpy().flatten()),axis=0 )\n",
    "\n",
    "\n",
    "# how many are there?\n",
    "print(f'There are {len(allbiases)} bias parameters.')\n",
    "print(f'There are {len(allweight)} weight parameters.')\n",
    "\n",
    "\n",
    "# show their histograms\n",
    "fig,ax = plt.subplots(1,3,figsize=(18,4))\n",
    "\n",
    "ax[0].hist(allbiases,40)\n",
    "ax[0].set_title('Histogram of initial biases')\n",
    "\n",
    "\n",
    "ax[1].hist(allweight,40)\n",
    "ax[1].set_title('Histogram of initial weights')\n",
    "\n",
    "\n",
    "\n",
    "# collect histogram data to show as line plots\n",
    "yB,xB = np.histogram(allbiases,30)\n",
    "yW,xW = np.histogram(allweight,30)\n",
    "\n",
    "ax[2].plot((xB[1:]+xB[:-1])/2,yB/np.sum(yB),label='Bias')\n",
    "ax[2].plot((xW[1:]+xW[:-1])/2,yW/np.sum(yW),label='Weight')\n",
    "ax[2].set_title('Density estimate for both')\n",
    "ax[2].legend()\n",
    "\n",
    "\n",
    "# plot adjustments common to all subplots\n",
    "for i in range(3):\n",
    "  ax[i].set_xlabel('Initial value')\n",
    "  ax[i].set_ylabel('Count')\n",
    "ax[2].set_ylabel('Probability')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1ec810",
   "metadata": {},
   "source": [
    "# Layer-specific distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,4))\n",
    "\n",
    "for p in net.named_parameters():\n",
    "\n",
    "  # get the data and compute their histogram\n",
    "  thesedata = p[1].data.numpy().flatten()\n",
    "  y,x = np.histogram(thesedata,10)\n",
    "\n",
    "  # for the bias\n",
    "  if 'bias' in p[0]:\n",
    "    ax[0].plot((x[1:]+x[:-1])/2,y/np.sum(y),label='%s bias (N=%g)'%(p[0][:-5],len(thesedata)))\n",
    "\n",
    "  # for the weights\n",
    "  elif 'weight' in p[0]:\n",
    "    ax[1].plot((x[1:]+x[:-1])/2,y/np.sum(y),label='%s weight (N=%g)'%(p[0][:-7],len(thesedata)))\n",
    "\n",
    "\n",
    "\n",
    "ax[0].set_title('Biases per layer')\n",
    "ax[0].legend()\n",
    "ax[1].set_title('Weights per layer')\n",
    "ax[1].legend(bbox_to_anchor=(1,1),loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22ce991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's up with the weird output bias distribution??\n",
    "print( net.output.bias.data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the docstring for linear layers\n",
    "nn.Linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test whether the numbers match our prediction from the formula\n",
    "\n",
    "# empirical bias range\n",
    "biasrange = [ torch.min(net.fc1.bias.data).item(), torch.max(net.fc1.bias.data).item() ]\n",
    "biascount = len(net.fc1.bias.data)\n",
    "\n",
    "# theoretical expected value\n",
    "sigma = np.sqrt(1/biascount)\n",
    "\n",
    "# drum rolllllll.....\n",
    "print('Theoretical sigma = ' + str(sigma))\n",
    "print('Empirical range = ' + str(biasrange))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cdc21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "091584dd",
   "metadata": {},
   "source": [
    "# Now to initialize the weights using the Xavier method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new instance of the model\n",
    "net = thenet()\n",
    "\n",
    "# change the weights (leave biases as Kaiming [default])\n",
    "for p in net.named_parameters():\n",
    "  if 'weight' in p[0]:\n",
    "    nn.init.xavier_normal_(p[1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: it is unconventional to have Kaiming biases and Xavier weights.\n",
    "# Scroll up and re-run the previous weights visualization cells with the new network.\n",
    "# Then continue below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test whether the numbers match our prediction from the formula\n",
    "\n",
    "# empirical weight standard deviation\n",
    "weightvar   = torch.var(net.fc1.weight.data.flatten()).item()\n",
    "weightcount = len(net.fc1.weight.data)\n",
    "\n",
    "# theoretical expected value\n",
    "sigma2 = 2 / (weightcount+weightcount)\n",
    "\n",
    "# drum rolllllll.....\n",
    "print('Theoretical sigma = ' + str(sigma2))\n",
    "print('Empirical variance = ' + str(weightvar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a776b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaeaf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: There are several other weights initialization methods availabe in PyTorch.\n",
    "#       See https://pytorch.org/docs/stable/nn.init.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159d3ba",
   "metadata": {},
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df8d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Explore the weight initialization options using PyTorch's functions (nn.init.<method>). \n",
    "#    For example: apply Xavier-uniform, Kaiming, constant (this is what we did in the first video of this section).\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
