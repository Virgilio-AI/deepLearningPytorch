{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417275c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 19/November/2022 - Saturday\n",
    "# Author: Virgilio Murillo Ochoa\n",
    "# personal github: Virgilio-AI\n",
    "# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203\n",
    "# contact: virgiliomurilloochoa1@gmail.com\n",
    "# web: virgiliomurillo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ade39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4ee143",
   "metadata": {},
   "source": [
    "# Sample problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febbd34",
   "metadata": {},
   "source": [
    "### Convolve an image of size 1x256x256 to produce a 1x252x84 result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec14c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "inChans  = 1 # RGB\n",
    "imsize   = [256,256]\n",
    "outChans = 1\n",
    "krnSize  = 7 # should be an odd number\n",
    "stride   = (1,3)\n",
    "padding  = 1\n",
    "\n",
    "# create the instance\n",
    "c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n",
    "\n",
    "# create an image\n",
    "img = torch.rand(1,inChans,imsize[0],imsize[1])\n",
    "\n",
    "# run convolution and compute its shape\n",
    "resimg = c(img)\n",
    "empSize = torch.squeeze(resimg).shape\n",
    "\n",
    "# compute the size of the result according to the formula\n",
    "expectSize = np.array([outChans,0,0],dtype=int)\n",
    "expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n",
    "expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n",
    "\n",
    "# check the size of the output\n",
    "print(f'Expected size: {expectSize}')\n",
    "print(f'Empirical size: {list(empSize)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745f27c",
   "metadata": {},
   "source": [
    "# Real problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822543d",
   "metadata": {},
   "source": [
    "### 1) Convolve an image of size 3x64x64 to produce a 10x28x28 result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70adc412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "inChans  = \n",
    "imsize   = \n",
    "outChans = \n",
    "krnSize  = \n",
    "stride   = \n",
    "padding  = \n",
    "\n",
    "# create the instance\n",
    "c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n",
    "\n",
    "# create an image\n",
    "img = torch.rand(1,inChans,imsize[0],imsize[1])\n",
    "\n",
    "# run convolution and compute its shape\n",
    "resimg = c(img)\n",
    "empSize = torch.squeeze(resimg).shape\n",
    "\n",
    "# compute the size of the result according to the formula\n",
    "expectSize = np.array([outChans,0,0],dtype=int)\n",
    "expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n",
    "expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n",
    "\n",
    "# check the size of the output\n",
    "print(f'Expected size: {expectSize}')\n",
    "print(f'Empirical size: {list(empSize)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf8ab52",
   "metadata": {},
   "source": [
    "### 2) Convolve an image of size 3x196x96 to produce a 5x66x49 result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b25161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "inChans  = \n",
    "imsize   = \n",
    "outChans = \n",
    "krnSize  = \n",
    "stride   = \n",
    "padding  = \n",
    "\n",
    "# create the instance\n",
    "c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n",
    "\n",
    "# create an image\n",
    "img = torch.rand(1,inChans,imsize[0],imsize[1])\n",
    "\n",
    "# run convolution and compute its shape\n",
    "resimg = c(img)\n",
    "empSize = torch.squeeze(resimg).shape\n",
    "\n",
    "# compute the size of the result according to the formula\n",
    "expectSize = np.array([outChans,0,0],dtype=int)\n",
    "expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n",
    "expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n",
    "\n",
    "# check the size of the output\n",
    "print(f'Expected size: {expectSize}')\n",
    "print(f'Empirical size: {list(empSize)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010207b9",
   "metadata": {},
   "source": [
    "### 3) Convolve an image of size 1x32x32 to produce a 6x28x28 result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d51841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: these dimensions are the input -> first hidden layer of the famous LeNet-5\n",
    "\n",
    "# parameters\n",
    "inChans  = \n",
    "imsize   = \n",
    "outChans = \n",
    "krnSize  = \n",
    "stride   = \n",
    "padding  = \n",
    "\n",
    "# create the instance\n",
    "c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n",
    "\n",
    "# create an image\n",
    "img = torch.rand(1,inChans,imsize[0],imsize[1])\n",
    "\n",
    "# run convolution and compute its shape\n",
    "resimg = c(img)\n",
    "empSize = torch.squeeze(resimg).shape\n",
    "\n",
    "# compute the size of the result according to the formula\n",
    "expectSize = np.array([outChans,0,0],dtype=int)\n",
    "expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n",
    "expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n",
    "\n",
    "# check the size of the output\n",
    "print(f'Expected size: {expectSize}')\n",
    "print(f'Empirical size: {list(empSize)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37517dc",
   "metadata": {},
   "source": [
    "### 4) Convolve an image of size 3x227x227 to produce a 96x55x55 result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc8f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: these dimensions are the input -> first hidden layer of the famous AlexNet\n",
    "\n",
    "# parameters\n",
    "inChans  = \n",
    "imsize   = \n",
    "outChans = \n",
    "krnSize  = \n",
    "stride   = \n",
    "padding  = \n",
    "\n",
    "# create the instance\n",
    "c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n",
    "\n",
    "# create an image\n",
    "img = torch.rand(1,inChans,imsize[0],imsize[1])\n",
    "\n",
    "# run convolution and compute its shape\n",
    "resimg = c(img)\n",
    "empSize = torch.squeeze(resimg).shape\n",
    "\n",
    "# compute the size of the result according to the formula\n",
    "expectSize = np.array([outChans,0,0],dtype=int)\n",
    "expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n",
    "expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n",
    "\n",
    "# check the size of the output\n",
    "print(f'Expected size: {expectSize}')\n",
    "print(f'Empirical size: {list(empSize)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac13b312",
   "metadata": {},
   "source": [
    "### 5) Convolve an image of size 3x224x224 to produce a 64x224x224 result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec325df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: these dimensions are the input -> first hidden layer of the famous VGG-16\n",
    "\n",
    "# parameters\n",
    "inChans  = \n",
    "imsize   = \n",
    "outChans = \n",
    "krnSize  = \n",
    "stride   = \n",
    "padding  = \n",
    "\n",
    "# create the instance\n",
    "c = nn.Conv2d(inChans,outChans,krnSize,stride,padding)\n",
    "\n",
    "# create an image\n",
    "img = torch.rand(1,inChans,imsize[0],imsize[1])\n",
    "\n",
    "# run convolution and compute its shape\n",
    "resimg = c(img)\n",
    "empSize = torch.squeeze(resimg).shape\n",
    "\n",
    "# compute the size of the result according to the formula\n",
    "expectSize = np.array([outChans,0,0],dtype=int)\n",
    "expectSize[1] = np.floor( (imsize[0]+2*padding-krnSize)/stride[0] ) + 1\n",
    "expectSize[2] = np.floor( (imsize[1]+2*padding-krnSize)/stride[1] ) + 1\n",
    "\n",
    "# check the size of the output\n",
    "print(f'Expected size: {expectSize}')\n",
    "print(f'Empirical size: {list(empSize)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee0fe9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767833a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d29c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9503c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa11e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db73dc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a41f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590633d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca0115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c50029d7",
   "metadata": {},
   "source": [
    "# Answers (don't cheat!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92927df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)\n",
    "inChans  = 3\n",
    "imsize   = [64,64]\n",
    "outChans = 10\n",
    "krnSize  = 9\n",
    "stride   = (2,2)\n",
    "padding  = 0\n",
    "\n",
    "# 2)\n",
    "inChans  = 3\n",
    "imsize   = [196,96]\n",
    "outChans = 5\n",
    "krnSize  = 5\n",
    "stride   = (3,2)\n",
    "padding  = 3\n",
    "\n",
    "# 3)\n",
    "inChans  = 1\n",
    "imsize   = [32,32]\n",
    "outChans = 6\n",
    "krnSize  = 5\n",
    "stride   = (1,1)\n",
    "padding  = 0\n",
    "\n",
    "# 4)\n",
    "inChans  = 3\n",
    "imsize   = [227,227]\n",
    "outChans = 96\n",
    "krnSize  = 11\n",
    "stride   = (4,4)\n",
    "padding  = 1\n",
    "\n",
    "# 5)\n",
    "inChans  = 3\n",
    "imsize   = [224,224]\n",
    "outChans = 64\n",
    "krnSize  = 3\n",
    "stride   = (1,1)\n",
    "padding  = 1"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
