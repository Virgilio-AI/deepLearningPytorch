{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe01856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 19/November/2022 - Saturday\n",
    "# Author: Virgilio Murillo Ochoa\n",
    "# personal github: Virgilio-AI\n",
    "# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203\n",
    "# contact: virgiliomurilloochoa1@gmail.com\n",
    "# web: virgiliomurillo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e61b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"manually\" in numpy\n",
    "\n",
    "# the list of numbers\n",
    "z = [1,2,3]\n",
    "\n",
    "# compute the softmax result\n",
    "num = np.exp(z)\n",
    "den = np.sum( np.exp(z) )\n",
    "sigma = num / den\n",
    "\n",
    "print(sigma)\n",
    "print(np.sum(sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904e4574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat with some random integers\n",
    "z = np.random.randint(-5,high=15,size=25)\n",
    "print(z)\n",
    "\n",
    "# compute the softmax result\n",
    "num = np.exp(z)\n",
    "den = np.sum( num )\n",
    "sigma = num / den\n",
    "\n",
    "# compare\n",
    "plt.plot(z,sigma,'ko')\n",
    "plt.xlabel('Original number (z)')\n",
    "plt.ylabel('Softmaxified $\\sigma$')\n",
    "plt.yscale('log')\n",
    "plt.title('$\\sum\\sigma$ = %g' %np.sum(sigma))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0348dc",
   "metadata": {},
   "source": [
    "# Using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0729f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly more involved using torch.nn\n",
    "\n",
    "# create an instance of the softmax activation class\n",
    "softfun = nn.Softmax(dim=0)\n",
    "\n",
    "# then apply the data to that function\n",
    "sigmaT = softfun( torch.Tensor(z) )\n",
    "\n",
    "# now we get the results\n",
    "print(sigmaT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2341ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show that they are the same\n",
    "plt.plot(sigma,sigmaT,'ko')\n",
    "plt.xlabel('\"Manual\" softmax')\n",
    "plt.ylabel('Pytorch nn.Softmax')\n",
    "plt.title(f'The two methods correlate at r={np.corrcoef(sigma,sigmaT)[0,1]}')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
