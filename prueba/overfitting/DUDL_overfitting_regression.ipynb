{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d8c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 19/November/2022 - Saturday\n",
    "# Author: Virgilio Murillo Ochoa\n",
    "# personal github: Virgilio-AI\n",
    "# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203\n",
    "# contact: virgiliomurilloochoa1@gmail.com\n",
    "# web: virgiliomurillo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1954f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e7bc85",
   "metadata": {},
   "source": [
    "# Create the data and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "x = torch.randn(N,1)\n",
    "y = x + torch.randn(N,1)\n",
    "\n",
    "# and plot\n",
    "plt.plot(x,y,'s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49a247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "ANNreg = nn.Sequential(\n",
    "    nn.Linear(1,1),  # input layer\n",
    "    nn.ReLU(),       # activation function\n",
    "    nn.Linear(1,1)   # output layer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e145c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model meta-parameters\n",
    "\n",
    "learningRate = .05\n",
    "\n",
    "# loss function\n",
    "lossfun = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(ANNreg.parameters(),lr=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e077dd",
   "metadata": {},
   "source": [
    "# Select data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2891f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select training data (note the hard-coded N!)\n",
    "trainidx  = np.random.choice(range(N),80,replace=False) # random indices\n",
    "trainBool = np.zeros(N,dtype=bool) # initialize vector of Falses'\n",
    "trainBool[trainidx] = True # set selected samples to True\n",
    "\n",
    "# show the sizes\n",
    "print(x[trainBool].shape)\n",
    "print(x[~trainBool].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766658e4",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6a4f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "numepochs = 500\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "  # forward pass\n",
    "  yHat = ANNreg(x[trainBool])\n",
    "\n",
    "  # compute loss\n",
    "  loss = lossfun(yHat,y[trainBool])\n",
    "\n",
    "  # backprop\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ebda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the losses\n",
    "\n",
    "# compute losses of the TEST set\n",
    "predYtest = ANNreg(x[~trainBool])\n",
    "testloss = (predYtest-y[~trainBool]).pow(2).mean()\n",
    "\n",
    "# print out final TRAIN loss and TEST loss\n",
    "print(f'Final TRAIN loss: {loss.detach():.2f}')\n",
    "print(f'Final TEST loss: {testloss.detach():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c532651",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot the data\n",
    "\n",
    "# predictions for final training run\n",
    "predYtrain = ANNreg(x[trainBool]).detach().numpy()\n",
    "\n",
    "# now plot\n",
    "plt.plot(x,y,'k^',label='All data')\n",
    "plt.plot(x[trainBool], predYtrain,\n",
    "         'bs',markerfacecolor='w',label='Training pred.')\n",
    "plt.plot(x[~trainBool],predYtest.detach(),\n",
    "         'ro',markerfacecolor='w',label='Test pred.')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d393c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b84cd26f",
   "metadata": {},
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41bcd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) The train/test split is currently hard-coded to be 80/20 (note the number \"80\"). This is bad coding style, because\n",
    "#    if you change the number of datapoints from N=100 to N=10000, then we're still only training on 80 samples and testing\n",
    "#    on 10000-80=9920 samples. Change how the variable trainBool is created so that it always trains on 80% of the data,\n",
    "#    regardless of the dataset size.\n",
    "# \n",
    "# 2) Re-write this code to use scikitlearn and/or DataLoader instead of manually separating the data into train/test.\n",
    "# \n",
    "# 3) Do we really need 500 epochs to train the model? To find out, add code to the training loop to compute the MSEloss \n",
    "#    for the train and test data on each iteration during training. Then plot the train and test error as a function of\n",
    "#    training epoch. What is your evaluation of an appropriate amount of training for this model/dataset?\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
