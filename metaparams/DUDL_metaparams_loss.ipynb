{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075fc148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 19/November/2022 - Saturday\n",
    "# Author: Virgilio Murillo Ochoa\n",
    "# personal github: Virgilio-AI\n",
    "# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203\n",
    "# contact: virgiliomurilloochoa1@gmail.com\n",
    "# web: virgiliomurillo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d704b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6926f2a",
   "metadata": {},
   "source": [
    "# Mean-squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b58af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "lossfunMSE = nn.MSELoss()\n",
    "\n",
    "# create predictions and real answer\n",
    "yHat = torch.linspace(-2,2,101)\n",
    "y = torch.tensor(.5)\n",
    "\n",
    "# compute MSE loss function\n",
    "L = np.zeros(101)\n",
    "for i,yy in enumerate(yHat):\n",
    "  L[i] = lossfunMSE(yy,y)\n",
    "\n",
    "plt.plot(yHat,L,label='Loss')\n",
    "plt.plot([y,y],[0,np.max(L)],'r--',label='True value')\n",
    "plt.xlabel('Predicted value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be232734",
   "metadata": {},
   "source": [
    "# Binary cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "lossfunBCE = nn.BCELoss()\n",
    "\n",
    "# create predictions and real answer\n",
    "yHat = torch.linspace(.001,.999,101)\n",
    "y1 = torch.tensor(0.)\n",
    "y2 = torch.tensor(1.)\n",
    "\n",
    "# compute MSE loss function\n",
    "L = np.zeros((101,2))\n",
    "for i,yy in enumerate(yHat):\n",
    "  L[i,0] = lossfunBCE(yy,y1) # 0 is the correct answer\n",
    "  L[i,1] = lossfunBCE(yy,y2) # 1 is the correct answer\n",
    "\n",
    "plt.plot(yHat,L)\n",
    "plt.xlabel('Predicted value')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['correct=0','correct=1'])\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023048be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# The example above shows data already in probabilities. Raw outputs will need to be converted to probabilities:\n",
    "\n",
    "# \"raw\" output of a model\n",
    "yHat = torch.tensor(2.)\n",
    "print(lossfunBCE(yHat,y2))\n",
    "\n",
    "# convert to prob via sigmoid\n",
    "sig = nn.Sigmoid()\n",
    "print(lossfunBCE( sig(yHat) ,y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84da452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, PyTorch recommends using a single function that incorporates sigmoid+BCE due to increased numerical stability.\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html?highlight=nn%20bcewithlogitsloss#torch.nn.BCEWithLogitsLoss\n",
    "\n",
    "\n",
    "# Thus, the recommended way to do it:\n",
    "lossfunBCE = nn.BCEWithLogitsLoss()\n",
    "yHat = torch.tensor(2.)\n",
    "print(lossfunBCE(yHat,y2))\n",
    "\n",
    "# In toy examples, numerical accuracy usually isn't a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb3d4f",
   "metadata": {},
   "source": [
    "# Categorical cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "lossfunCCE = nn.CrossEntropyLoss()\n",
    "\n",
    "# vector of output layer (pre-softmax)\n",
    "yHat = torch.tensor([[1.,4,3]])\n",
    "\n",
    "for i in range(3):\n",
    "  correctAnswer = torch.tensor([i])\n",
    "  thisloss = lossfunCCE(yHat,correctAnswer).item()\n",
    "  print( 'Loss when correct answer is %g: %g' %(i,thisloss) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651b684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat using pre-softmaxified output\n",
    "sm = nn.Softmax(dim=1)\n",
    "yHat_sm = sm(yHat)\n",
    "\n",
    "for i in range(3):\n",
    "  correctAnswer = torch.tensor([i])\n",
    "  thisloss = lossfunCCE(yHat_sm,correctAnswer).item()\n",
    "  print( 'Loss when correct answer is %g: %g' %(i,thisloss) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare raw, softmax, and log-softmax outputs\n",
    "sm = nn.LogSoftmax(dim=1)\n",
    "yHat_logsm = sm(yHat)\n",
    "\n",
    "# print them\n",
    "print(yHat)\n",
    "print(yHat_sm)\n",
    "print(yHat_logsm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3417c",
   "metadata": {},
   "source": [
    "# Creating your own custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd92d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLoss(nn.Module): # inherent info from nn.Module\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "      \n",
    "  def forward(self,x,y):\n",
    "    loss = torch.abs(x-y)\n",
    "    return loss\n",
    "\n",
    "# test it out!\n",
    "lfun = myLoss()\n",
    "lfun(torch.tensor(4),torch.tensor(5.2))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
