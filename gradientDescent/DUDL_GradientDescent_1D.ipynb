{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 19/November/2022 - Saturday\n",
    "# Author: Virgilio Murillo Ochoa\n",
    "# personal github: Virgilio-AI\n",
    "# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203\n",
    "# contact: virgiliomurilloochoa1@gmail.com\n",
    "# web: virgiliomurillo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d07512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594566ed",
   "metadata": {},
   "source": [
    "# Gradient descent in 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a1024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function (as a function)\n",
    "def fx(x):\n",
    "  return 3*x**2 - 3*x + 4\n",
    "\n",
    "# derivative function\n",
    "def deriv(x):\n",
    "  return 6*x - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the function and its derivative\n",
    "\n",
    "# define a range for x\n",
    "x = np.linspace(-2,2,2001)\n",
    "\n",
    "# plotting\n",
    "plt.plot(x,fx(x), x,deriv(x))\n",
    "plt.xlim(x[[0,-1]])\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend(['y','dy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493842c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86322c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random starting point\n",
    "localmin = np.random.choice(x,1)\n",
    "print(localmin)\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = .01\n",
    "training_epochs = 100\n",
    "\n",
    "# run through training\n",
    "for i in range(training_epochs):\n",
    "  grad = deriv(localmin)\n",
    "  localmin = localmin - learning_rate*grad\n",
    "\n",
    "localmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a68a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "\n",
    "plt.plot(x,fx(x), x,deriv(x))\n",
    "plt.plot(localmin,deriv(localmin),'ro')\n",
    "plt.plot(localmin,fx(localmin),'ro')\n",
    "\n",
    "plt.xlim(x[[0,-1]])\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend(['f(x)','df','f(x) min'])\n",
    "plt.title('Empirical local minimum: %s'%localmin[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d502f",
   "metadata": {},
   "source": [
    "# Store the model parameters and outputs on each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da468c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# random starting point\n",
    "localmin = np.random.choice(x,1)\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = .01\n",
    "training_epochs = 100\n",
    "\n",
    "# run through training and store all the results\n",
    "modelparams = np.zeros((training_epochs,2))\n",
    "for i in range(training_epochs):\n",
    "  grad = deriv(localmin)\n",
    "  localmin = localmin - learning_rate*grad\n",
    "  modelparams[i,:] = localmin,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2002e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the gradient over iterations\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,4))\n",
    "\n",
    "for i in range(2):\n",
    "  ax[i].plot(modelparams[:,i],'o-')\n",
    "  ax[i].set_xlabel('Iteration')\n",
    "  ax[i].set_title(f'Final estimated minimum: {localmin[0]:.5f}')\n",
    "\n",
    "ax[0].set_ylabel('Local minimum')\n",
    "ax[1].set_ylabel('Derivative')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c891a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95f4e907",
   "metadata": {},
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0bd50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Most often in DL, the model trains for a set number of iterations, which is what we do here. But there are other ways\n",
    "#    of defining how long the training lasts. Modify the code so that training ends when the derivative is smaller than \n",
    "#    some threshold, e.g., 0.1. Make sure your code is robust for negative derivatives.\n",
    "# \n",
    "# 2) Does this change to the code produce a more accurate result? What if you change the stopping threshold?\n",
    "# \n",
    "# 3) Can you think of any potential problems that might arise when the stopping criterion is based on the derivative \n",
    "#    instead of a specified number of training epochs?\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
