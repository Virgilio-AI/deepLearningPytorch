{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 17/November/2022 - Thursday\n",
    "# Author: Virgilio Murillo Ochoa\n",
    "# personal github: Virgilio-AI\n",
    "# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203\n",
    "# contact: data_scientist@virgiliomurillo.com\n",
    "# web: virgiliomurillo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f52566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# NEW! for doing PCA on the model output\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3921a72",
   "metadata": {},
   "source": [
    "# Import and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset (comes with colab!)\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# we'll use the labels for matching with the latent code\n",
    "labels = data[:,0]\n",
    "data   = data[:,1:]\n",
    "\n",
    "# normalize the data to a range of [0 1]\n",
    "dataNorm = data / np.max(data)\n",
    "\n",
    "# convert to tensor\n",
    "dataT = torch.tensor( dataNorm ).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7395fac",
   "metadata": {},
   "source": [
    "# Create the DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "def createTheMNISTAE():\n",
    "\n",
    "  class aenet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      ### input layer\n",
    "      self.input = nn.Linear(784,150)\n",
    "      \n",
    "      ### encoder layer\n",
    "      self.enc = nn.Linear(150,15)\n",
    "\n",
    "      ### latent layer\n",
    "      self.lat = nn.Linear(15,150)\n",
    "\n",
    "      ### decoder layer\n",
    "      self.dec = nn.Linear(150,784)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self,x):\n",
    "      x = F.relu( self.input(x) )\n",
    "      \n",
    "      # NEW! output the hidden-layer activation\n",
    "      codex = F.relu( self.enc(x) )\n",
    "      \n",
    "      x = F.relu( self.lat(codex) )\n",
    "      y = torch.sigmoid( self.dec(x) )\n",
    "      return y,codex\n",
    "  \n",
    "  # create the model instance\n",
    "  net = aenet()\n",
    "  \n",
    "  # loss function\n",
    "  lossfun = nn.MSELoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.Adam(net.parameters(),lr=.001)\n",
    "\n",
    "  return net,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b981b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model with a bit of data\n",
    "net,lossfun,optimizer = createTheMNISTAE()\n",
    "\n",
    "X = dataT[:5,:]\n",
    "yHat = net(X)\n",
    "\n",
    "print('Input shape:')\n",
    "print(X.shape)\n",
    "print(' ')\n",
    "\n",
    "# yHat is now a tuple\n",
    "print(type(yHat),len(yHat))\n",
    "print(' ')\n",
    "\n",
    "print('Shape of model output:')\n",
    "print(yHat[0].shape)\n",
    "print(' ')\n",
    "\n",
    "print('Shape of encoding layer output:')\n",
    "print(yHat[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e612d740",
   "metadata": {},
   "source": [
    "# Create a function that trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e06bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function2trainTheModel():\n",
    "\n",
    "  # number of epochs\n",
    "  numepochs = 10000\n",
    "  \n",
    "  # create a new model\n",
    "  net,lossfun,optimizer = createTheMNISTAE()\n",
    "\n",
    "  # initialize losses\n",
    "  losses = torch.zeros(numepochs)\n",
    "\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # select a random set of images\n",
    "    randomidx = np.random.choice(dataT.shape[0],size=32)\n",
    "    X = dataT[randomidx,:]\n",
    "\n",
    "    # forward pass and loss\n",
    "    yHat = net(X)[0] # NEW! here we only care about the final model output\n",
    "    loss = lossfun(yHat,X)\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # losses in this epoch\n",
    "    losses[epochi] = loss.item()\n",
    "  # end epochs\n",
    "\n",
    "  # function output\n",
    "  return losses,net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e8f3ec",
   "metadata": {},
   "source": [
    "# Run the model and show the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "losses,net = function2trainTheModel()\n",
    "print(f'Final loss: {losses[-1]:.4f}')\n",
    "\n",
    "# visualize the losses\n",
    "plt.plot(losses,'.-')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Model loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002d5dbf",
   "metadata": {},
   "source": [
    "# Inspect the latent \"code\" of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the latent layer\n",
    "\n",
    "# push through the entire dataset\n",
    "yHat,latent = net(dataT)\n",
    "\n",
    "# print sizes\n",
    "print(yHat.shape)\n",
    "print(latent.shape)\n",
    "\n",
    "# what does it look like?\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ax[0].hist(latent.flatten().detach(),100)\n",
    "ax[0].set_xlabel('Latent activation value')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_title('Distribution of latent units activations')\n",
    "\n",
    "ax[1].imshow(latent.detach(),aspect='auto',vmin=0,vmax=10)\n",
    "ax[1].set_xlabel('Latent node')\n",
    "ax[1].set_ylabel('Image number')\n",
    "ax[1].set_title('All latent activations')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23212732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average latent activation for each digit type\n",
    "\n",
    "# initialize output matrix (latent shape by 10 digits)\n",
    "sourcecode = np.zeros((latent.shape[1],10))\n",
    "\n",
    "\n",
    "# loop over digit categories\n",
    "for i in range(10):\n",
    "\n",
    "  # find all pictures of this category\n",
    "  digidx = np.where(labels==i)\n",
    "\n",
    "  # average the latent layer output\n",
    "  sourcecode[:,i] = torch.mean(latent[digidx,:],axis=1).detach()\n",
    "\n",
    "\n",
    "# let's see what it looks like!\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.plot(sourcecode,'s-')\n",
    "plt.legend(range(10),loc=(1.01,.4))\n",
    "plt.xticks(range(15))\n",
    "plt.xlabel('Latent node number')\n",
    "plt.ylabel('Activation')\n",
    "plt.title(\"The model's internal representation of the numbers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e626e",
   "metadata": {},
   "source": [
    "# Explore the reduced-compressed space with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38fbbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and fit the PCA\n",
    "pcaData = PCA(n_components=15).fit(data) # 15 components to match latent, but it's just to speed computation time\n",
    "pcaCode = PCA(               ).fit(latent.detach())\n",
    "\n",
    "\n",
    "# plot the eigenspectra (scree plot)\n",
    "plt.plot(100*pcaData.explained_variance_ratio_,'s-',label='Data PCA')\n",
    "plt.plot(100*pcaCode.explained_variance_ratio_,'o-',label='Code PCA')\n",
    "plt.xlabel('Components')\n",
    "plt.ylabel('Percent variance explained')\n",
    "plt.title('PCA scree plot')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb2b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the projection of the data onto the PC axes\n",
    "scoresData = pcaData.fit_transform(data)\n",
    "scoresCode = pcaCode.fit_transform(latent.detach())\n",
    "\n",
    "# plot the data separately per number\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "for lab in range(10):\n",
    "  ax[0].plot(scoresData[labels==lab,0],scoresData[labels==lab,1],'o',markersize=3,alpha=.4)\n",
    "  ax[1].plot(scoresCode[labels==lab,0],scoresCode[labels==lab,1],'o',markersize=3,alpha=.4)\n",
    "\n",
    "for i in range(2):\n",
    "  ax[i].set_xlabel('PC1 projection')\n",
    "  ax[i].set_ylabel('PC2 projection')\n",
    "  ax[i].legend(range(10))\n",
    "\n",
    "ax[0].set_title('PCA of data')\n",
    "ax[1].set_title('PCA of latent code')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd208d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is not important! It's just the code I used to make the figure in the slide. I decided to leave it here FYI.\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(15,3))\n",
    "\n",
    "ax[0].imshow(dataT[0,:].view(28,28),cmap='gray')\n",
    "\n",
    "ax[1].plot(dataT[0,:],'ks')\n",
    "ax[1].set_xlabel('Pixels (vectorized)')\n",
    "ax[1].set_ylabel('Intensity value')\n",
    "\n",
    "ax[2].plot(latent[0,:].detach(),'ks')\n",
    "ax[2].set_xlabel('Latent units')\n",
    "ax[2].set_ylabel('Activation (a.u.)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c30b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7347fd6d",
   "metadata": {},
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380850b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Are you surprised that the latent activations (e.g., from the histogram) are all non-negative? Is that because of \n",
    "#    the image normalization, or what is causing those values to be all non-negative?\n",
    "# \n",
    "# 2) Averages don't tell the whole story. Redraw the \"Model's internal representation\" line plot but using standard \n",
    "#    deviation instead of mean. This graph will tell you if any numbers, or units, have particularly higher variability\n",
    "#    than others. Is this the case, and does the std plot give you any more insight into the model's learned representation?\n",
    "# \n",
    "# 3) The PC-space plots are tricky to interpret: This is a 15-dimensional space but 13 dimensions are projected onto two.\n",
    "#    It's possible that the numbers are better separated in other dimensions, just like a 2D photograph of someone standing\n",
    "#    behind a tree makes them inseparable whereas they are separable in the original 3D space. Modify the plot to show\n",
    "#    PC dimensions 2&3 instead of 1&2. \n",
    "# "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
