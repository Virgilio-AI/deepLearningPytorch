{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f6985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 19/November/2022 - Saturday\n",
    "# Author: Virgilio Murillo Ochoa\n",
    "# personal github: Virgilio-AI\n",
    "# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203\n",
    "# contact: virgiliomurilloochoa1@gmail.com\n",
    "# web: virgiliomurillo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e8eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2833ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "\n",
    "nPerClust = 100\n",
    "blur = 1\n",
    "\n",
    "A = [  1,  3 ]\n",
    "B = [  1, -2 ]\n",
    "\n",
    "# generate data\n",
    "a = [ A[0]+np.random.randn(nPerClust)*blur , A[1]+np.random.randn(nPerClust)*blur ]\n",
    "b = [ B[0]+np.random.randn(nPerClust)*blur , B[1]+np.random.randn(nPerClust)*blur ]\n",
    "\n",
    "# true labels\n",
    "labels_np = np.vstack((np.zeros((nPerClust,1)),np.ones((nPerClust,1))))\n",
    "\n",
    "# concatanate into a matrix\n",
    "data_np = np.hstack((a,b)).T\n",
    "\n",
    "# convert to a pytorch tensor\n",
    "data = torch.tensor(data_np).float()\n",
    "labels = torch.tensor(labels_np).float()\n",
    "\n",
    "# show the data\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'bs')\n",
    "plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'ko')\n",
    "plt.title('The qwerties!')\n",
    "plt.xlabel('qwerty dimension 1')\n",
    "plt.ylabel('qwerty dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0330af2",
   "metadata": {},
   "source": [
    "# Functions to build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef5aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createANNmodel(learningRate):\n",
    "\n",
    "  # model architecture\n",
    "  ANNclassify = nn.Sequential(\n",
    "      nn.Linear(2,16),  # input layer\n",
    "      nn.ReLU(),        # activation unit\n",
    "      nn.Linear(16,1),  # hidden layer\n",
    "      nn.ReLU(),        # activation unit\n",
    "      nn.Linear(1,1),   # output unit\n",
    "      nn.Sigmoid(),     # final activation unit\n",
    "        )\n",
    "\n",
    "  # loss function\n",
    "  lossfun = nn.BCELoss() # but better to use BCEWithLogitsLoss\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.SGD(ANNclassify.parameters(),lr=learningRate)\n",
    "\n",
    "  # model output\n",
    "  return ANNclassify,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc1c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that trains the model\n",
    "\n",
    "# a fixed parameter\n",
    "numepochs = 1000\n",
    "\n",
    "def trainTheModel(ANNmodel):\n",
    "\n",
    "  # initialize losses\n",
    "  losses = torch.zeros(numepochs)\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # forward pass\n",
    "    yHat = ANNmodel(data)\n",
    "\n",
    "    # compute loss\n",
    "    loss = lossfun(yHat,labels)\n",
    "    losses[epochi] = loss\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "  \n",
    "  \n",
    "  # final forward pass\n",
    "  predictions = ANNmodel(data)\n",
    "    \n",
    "  # compute the predictions and report accuracy\n",
    "  # NOTE: Wasn't this \">0\" previously?!?!\n",
    "  totalacc = 100*torch.mean(((predictions>.5) == labels).float())\n",
    "  \n",
    "  return losses,predictions,totalacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2bb38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bd58fc0",
   "metadata": {},
   "source": [
    "# Test the new code by running it once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84508f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create everything\n",
    "ANNclassify,lossfun,optimizer = createANNmodel(.01)\n",
    "\n",
    "# run it\n",
    "losses,predictions,totalacc = trainTheModel(ANNclassify)\n",
    "\n",
    "# report accuracy\n",
    "print('Final accuracy: %g%%' %totalacc)\n",
    "\n",
    "\n",
    "# show the losses\n",
    "plt.plot(losses.detach(),'o',markerfacecolor='w',linewidth=.1)\n",
    "plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead74bbf",
   "metadata": {},
   "source": [
    "# Now for the real test (varying learning rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514c96fa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# learning rates\n",
    "learningrates = np.linspace(.001,.1,50)\n",
    "\n",
    "# initialize\n",
    "accByLR = []\n",
    "allLosses = np.zeros((len(learningrates),numepochs))\n",
    "\n",
    "\n",
    "# the loop\n",
    "for i,lr in enumerate(learningrates):\n",
    "  \n",
    "  # create and run the model\n",
    "  ANNclassify,lossfun,optimizer = createANNmodel(lr)\n",
    "  losses,predictions,totalacc = trainTheModel(ANNclassify)\n",
    "\n",
    "  # store the results\n",
    "  accByLR.append(totalacc)\n",
    "  allLosses[i,:] = losses.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig,ax = plt.subplots(1,2,figsize=(16,4))\n",
    "\n",
    "ax[0].plot(learningrates,accByLR,'s-')\n",
    "ax[0].set_xlabel('Learning rate')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_title('Accuracy by learning rate')\n",
    "\n",
    "ax[1].plot(allLosses.T)\n",
    "ax[1].set_title('Losses by learning rate')\n",
    "ax[1].set_xlabel('Epoch number')\n",
    "ax[1].set_ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f45c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "accByLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c376117",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(torch.tensor(accByLR)>70)/len(accByLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d28dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f1d7086",
   "metadata": {},
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b92571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) The code creates a model with 16 hidden units. Notice where the two \"16\"s appear when constructing the model. \n",
    "#    Recreate the model using 32 hidden units. Does that help with the issue of models getting stuck in local minima?\n",
    "# \n",
    "# 2) Adjust the code to create two hidden layers. The first hidden layer should have 16 hidden units and the second \n",
    "#    hidden layer shuold have 32 units. What needs to change in the code to make the numbers match to prevent coding errors?\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
